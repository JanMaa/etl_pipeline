{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code snippet you provided only imports the necessary modules and does not include any specific functionality or operations. It serves as a prerequisite for using the modules in further code that may follow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code sets the Google Cloud Platform (GCP) service account credentials, project ID, dataset ID, and table ID using environment variables. It then establishes a connection to Google BigQuery, executes a SQL query to select all rows from the specified table, and prints each row in the query result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.environ['GCP_CRED']\n",
    "project_id = os.environ['project_id']\n",
    "dataset_id = os.environ['dataset_id']\n",
    "table_id = os.environ['table_id']\n",
    "\n",
    "client = bigquery.Client()\n",
    "data_file_folder = os.environ['dataset_path']\n",
    "\n",
    "sql_query = f\"SELECT * FROM {project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "query_job = client.query(sql_query)\n",
    "\n",
    "for row in query_job.result():\n",
    "\tprint(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code defines a function called `table_reference` that takes in a project ID, dataset ID, and table ID as parameters. It creates a dataset reference and a table reference using the provided IDs, then prints the table reference and returns it as the output of the function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_reference(project_id, dataset_id, table_id):\n",
    "    dataset_ref = bigquery.DatasetReference(project_id, dataset_id)\n",
    "    table_ref = bigquery.TableReference(dataset_ref, table_id)\n",
    "    print(table_ref)\n",
    "    return table_ref"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code defines a function called `delete_dataset_tables` that takes a project ID and dataset ID as parameters. It uses the BigQuery client to retrieve a list of tables in the specified dataset, then iterates over each table and deletes it. Finally, it prints a message indicating that the tables have been deleted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dataset_tables(project_id, dataset_id):\n",
    "    tables = client.list_tables(f'{project_id}.{dataset_id}')\n",
    "    for table in tables:\n",
    "        client.delete_table(table)\n",
    "    print('Tables deleted.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code defines a function called `upload_csv` that uploads a CSV file into a specified table in Google BigQuery. It configures the schema for the table and the load job settings, then uses the BigQuery client to load the CSV file data into the table. It monitors the status of the upload job until it is done, then prints the result of the upload.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_csv(client, table_ref, csv_file):\n",
    "    # client.delete_table(table_ref, not_found_ok=True)\n",
    "\n",
    "    load_job_configuration = bigquery.LoadJobConfig()\n",
    "    load_job_configuration.schema = [\n",
    "        bigquery.SchemaField('Lead_ID', 'STRING', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Entry_Date', 'DATE', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Modify_Date', 'DATE', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Hours_Called', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Project_Cost_Hourly_Rate', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Campaign', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('Call_Status', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('New_Status', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('Contact_Status', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('Call_Stagging', 'DATE', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Total_Hours_Called', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Project_Cost_Rate', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Working_Days', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Registration_Goal', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('Campaign_Status', 'STRING', mode='REQUIRED')\n",
    "    ]\n",
    "\n",
    "    # load_job_configuration.autodetect = True\n",
    "    load_job_configuration.source_format = bigquery.SourceFormat.CSV\n",
    "    load_job_configuration.skip_leading_rows = 1\n",
    "    load_job_configuration.allow_quoted_newlines = True\n",
    "\n",
    "    with open(csv_file, 'rb') as source_file:\n",
    "        upload_job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            destination=table_ref,          \n",
    "            location='US',\n",
    "            job_config=load_job_configuration\n",
    "        )\n",
    "\n",
    "    while upload_job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        upload_job.reload()\n",
    "        print(upload_job.state)\n",
    "    print(upload_job.result())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code invokes the function `delete_dataset_tables` with the provided `project_id` and `dataset_id` as arguments. The function deletes all the tables within the specified dataset in Google BigQuery associated with the given project ID.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_dataset_tables(project_id, dataset_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code iterates over the files within the `data_file_folder` directory. It prints the name of each file and if a file starts with '2023', it processes it by creating a table reference based on the file name, then uploads the corresponding CSV file into the specified table using the `upload_csv` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(data_file_folder):\n",
    "    print(file)\n",
    "    if file.startswith('2023'):\n",
    "        print('Processing file: {0}'.format(file))\n",
    "        table_name = '_'.join(file.split('.')[:2])\n",
    "        csv_file = data_file_folder.joinpath(file)\n",
    "        print(csv_file)\n",
    "        table_ref = table_reference(project_id, dataset_id, table_name)\n",
    "        upload_csv(client, table_ref, csv_file)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
